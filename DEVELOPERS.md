# Notes for developers

## System requirements
- [`just`](https://github.com/casey/just)
- Docker
- recent version of Bash (see macOS notes)

## Local development environment
The `just` command provides a list of available recipes:
```
just list
```

Running any of the `just` commands that need it will set up a local environment and install dependencies.

## Testing

### Test categories

Tests are divided into the following categories.

<dl>
   <dt>unit</dt><dd>fast tests of small code units</dd>
   <dt>spec</dt><dd>tests generated from the ehrQL spec</dd>
   <dt>generative</dt><dd>query engine tests generated by Hypothesis</dd>
   <dt>acceptance</dt><dd>tests which demonstrate how ehrQL is used and check compatibility with real studies</dd>
   <dt>integration</dt><dd>tests of detailed code logic that require a database</dd>
   <dt>backend validation</dt><dd>tests which check that the backends correctly implement their contracts</dd>
   <dt>docker</dt><dd>tests of the ehrQL docker image</dd>
</dl>

Each category lives in its own directory (for example `tests/unit`) and has its own `just` command to run it (for
example `just test-unit`).

### Running tests

To run all tests, as they're run in CI (with code coverage, and the comprehensive
check for [generative tests](#generative-tests)):
```
just test-all
```

To run just one category of tests:
```
just test-<category>
```

For example:
```
just test-unit
```

Additional arguments can be passed to any test commands, for example:
```
just test-unit --verbose --durations=10
```

For maximum flexibility, the `test` command can be used to run individual test files or tests, or to do other clever things with `pytest`. It just delegates to `pytest`.
For example:
```
just test tests/integration/backends/test_tpp.py
```

There are further notes on using `pytest` in the wiki here:
https://github.com/opensafely-core/ehrql/wiki/Tips-for-using-pytest


#### Generative tests

The generative tests use Hypothesis to generate variable definitions (in the query model) and test data.
They then execute the resulting dataset definitions using the MSSQL, SQLite and in-memory query engines,
and check that the results are the same.

The GHAs use a fixed seed and run 200 examples, which is enough to ensure that all nodes are covered.  By default, tests run with `just test ...` or `pytest` use only 10 examples
to check that the infrastructure is basically working and avoid build flakiness due to finding new failure cases.

To get the benefit of the generative tests you need to run them at larger scale on your own dev box.
Use something like this:

```
GENTEST_EXAMPLES=10000 just test-generative
```

This generates 10k examples and takes ten or fifteen minutes to run.
When developing this, I (Ben) only ever saw one problem that took more than 10k examples to uncover, so that's pretty good as a check.

A scheduled GHA runs overnight with 40,000 examples to make sure that we're not missing anything.

You can get Hypothesis to dump statistics at the end of the run with `--hypothesis-show-statistics`,
or (more usefully) dump some of our own statistics about the generated data and queries by setting `GENTEST_DEBUG=t`.

When debugging a failure you'll probably want to reproduce it.

 * Hypothesis keeps some history and will _tend_ to re-run recent failure cases. So just re-running may be good enough.
 * The output from the failing test includes the examples in a form where they can be copy-pasted into the test code as arguments to a `@hyp.example()` decorator for the test.
   (You'll need to add some imports to get it to run.) This allows you to get the failure case running in a debugger
   (and also to get the example nicely formatted to help understand it).

Since the variable generation strategies are quite complex, it's hard to convince yourself that they give good coverage of the query space.
To help with this there is an optional assertion that the generative tests have included every query model operation at least once.
To enable this assertion set `GENTEST_COMPREHENSIVE=t`, like this:

```
GENTEST_COMPREHENSIVE=t GENTEST_EXAMPLES=200 just test-generative
```

Note that you need approximately 200 examples to have any chance of this passing.
`just test-all` runs with these parameters (unless passed a different value for `GENTEST_EXAMPLES`).

Hypothesis can generate query graphs that are very deeply nested; after 100 draws in a test example, hypothesis will return the example as invalid.  In order to avoid this, the
variable strategies check for a maximum depth and return a terminal node if the maximum depth is exceeded (A `SelectColumn` node for a series strategy, and a `SelectTable` or `SelectPatientTable` for a table strategy). The max depth defaults to 30 and can be overridden with environment variable `GENTEST_MAX_DEPTH`.

See the [generative tests documentation](tests/generative/README.md) for more details.

### Writing tests

Please think carefully about how to test code that you are adding or changing.
We think that test maintainability is a big risk for this system, so we're trying to be very deliberate about the kind of tests that we write.
You should follow these guidelines and raise it with the rest of the team for discussion if you think that they are problematic.

* _Major features in ehrQL or the ehrQL_ more widely should be motivated by a study used in an **acceptance** test.
  The number of such studies should be kept small in order that they don't be come a maintenance burden.
  The studies we use for the acceptance tests will need to be chosen carefully as representative of how we expect ehrQL to be used; they may be real studies or synthetic ones as appropriate.
* All _erhQL features_ should be covered by **spec** tests.
* Complex _query language logic_ that is not fully covered by the spec tests should be covered by **unit** tests.
  To avoid duplication, you should not write unit tests for logic that is adequately covered by the spec tests.
* The main _functionality of query engines_ will naturally be covered by **spec** tests, which are run against all the query engines.
* Complex _query engine logic_ that is not fully covered by the spec tests should be covered by **unit** or **integration** tests as appropriate.
  To avoid duplication, you should not write such tests for logic that is adequately covered by the spec tests.
* Basic operation of ehrQL as a _CLI tool_ is exercised by one trivial embedded **acceptance** test.
* Functionality of the _Docker image_ and how it invokes ehrQL are covered by a small set of **docker** tests.
* The adherence of _backends to contracts_ that they implement is automatically validated by **backend validation** tests.
* Where _backend tables_ do not map directly to the contracts that they implement, it may be helpful to write **integration** tests.
* All other _supporting logic_ should be covered by **unit** tests. Please avoid the temptation to cover this using acceptance, integration or docker tests that run the ehrQL end-to-end.

Contrary to practice in some quarters we allow disk access by **unit** tests because it doesn't seem to cause any significant slow-down in those tests at the moment.
We'll keep this under review.

#### Logging in tests

Logging is very verbose and is turned off by default in tests.  To turn it on, set the
environment variable `LOG_SQL=1` and pass the `-s` option to turn
off log capture in pytest.

```
LOG_SQL=1 just test-all -s
```

### Codebase structure

The files for test categories that target individual modules (for example **unit** and **integration** tests) are organized into roughly the same directory structure as the `databuilder` package itself.

Generally a module `databuilder.foo` will have a corresponding test file like `tests/unit/test_foo.py`.
However we do not stick slavishly to this: where appropriate we may collapse tests for submodules like `databuilder.foo.{bar,bam}` into a single test file like `tests/unit/test_foo.py`,
or break tests for a module like `databuilder.foo` into multiple test files like `tests/unit/foo/test_{one,another}_aspect.py`.

Test categories that run against the ehrQL as a whole or against multiple components (for example **spec** and **acceptance** tests) have their own internal structure.

### Code coverage

Our approach to code coverage is to fail the build with less than 100% coverage, but be reasonably liberal about allowing lines to be marked as not requiring coverage.
If you make a change that results in a newly un-covered line, you should make a good attempt to test it and expect to have to justify any omissions to PR reviewers;
but for genuinely hard or impossible to hit cases it's okay to mark them as `no cover`.

Any `no cover` pragmas should include a note explaining why the code can't be hit.
Common cases are configured in `pyproject.toml` with their own explanations.

### Test databases

For tests that need to run against a database, we run the database in a Docker container.
Each run of the tests starts the database if it's not already running _and then leaves it running_ at the end to speed up future runs.
(Each test cleans out the schema to avoid pollution.)

There is a `just` command to remove the database containers:
```
just remove-database-containers
```

### Displaying SQL queries

Set the environment variable `LOG_SQL=1` (or anything non-empty) to get all SQL queries logged to the console.  To get SQL queries in [test runs](#logging-in-tests), also use `-s` to turn
off log capture in pytest.

## macOS / Bash

Starting with version 4.0, Bash is licenced under GPLv3.
Because of this, macOS still ships with version 3.2, which is incompatible with some scripts in this repository.
We recommend using [homebrew](https://brew.sh/) to install a more recent version, ensure that the new version is on your `$PATH`, and restart your Terminal/shell session if necessary.

```bash
brew install bash
```


## Static Type Checking
We previously used [mypy](https://mypy.readthedocs.io/en/stable/) and type annotations to perform correctness checking of the code base.
However, we made the decision to remove this stack after finding it was not a good fit for large parts of the code base.

This does not mean we've abandoned type annotations entirely.
The `query_model` module still makes heavy use of them and implements its own runtime checking to enforce them.
And developers should feel free to use them wherever this aids clarity vs a docstring or a comment.

Dataclasses have also retained their annotations to avoid initialising all fields with None.


## Documentation

The documentation in this repository forms part of the main [OpenSAFELY documentation](https://github.com/opensafely/documentation).

It can also be built as a standalone documentation site with MkDocs to preview content changes, by running:

    just docs-serve

:warning: The documentation will look slightly different from OpenSAFELY's. Relative links to
sections of the main documentation outside of the /data-builder sections will not work (although
a scheduled [Github Action](https://github.com/opensafely-core/ehrql/actions/workflows/check-docs-links.yml) runs overnight to check them).

We may be able to improve this later, depending on the behaviour of the mkdocs plugin that
we use: see https://github.com/opensafely-core/ehrql/issues/1126

### Documentation redirects

These are handled in the main [OpenSAFELY documentation repository](https://github.com/opensafely/documentation).
If you need to redirect URLs —
and this should be fairly infrequent —
make any changes to the `_redirects` file in the main documentation repository,
and test them in a preview there.

### Structure

Databuilder documentation is located in the [docs](docs/) directory. Local configuration is
specified in the `mkdocs.yml` located at the repo root.

The `docs/` directory contains some files which are generated from the databuilder code and from
other documentation files. Specifically these are files at:

 - [docs/includes/generated_docs/](docs/includes/generated_docs/)
 - [docs/ehrql-tutorial-examples/outputs/](docs/ehrql-tutorial-examples/outputs/)

The process for generating these files is described below.

When the main OpenSAFELY documentation is built, it imports the databuilder `docs/` directory
and builds it within the main documentation site. This assumes that all generated documentation
has been updated already (see below for a description of pre-commit hooks and Github Actions
that mechanisms that check this happens).

#### Process for updating databuilder documentation

1. Developer makes changes to documentation files or code/tests that generate documentation
2. Changes committed; pre-commit hook ensures generated docs are up-to-date
3. PR opened; CI:
      - ensures generated docs are up to date
      - tests tutorial snippets
      - checks tutorial dataset definitions run successfully
      - check tutorial outputs are current
3. PR merged; CI:
      - triggers a deploy of the main OpenSAFELY documentation site
      - checks if the major version of the databuilder image has changed and updates it in the
        tutorial project.yaml
      - runs all tutorial dataset definitions with OpenSAFELY CLI
      - if the project.yaml or tutorial outputs have changed, opens a PR for the changes
4. On a schedule (nightly), CI:
      - checks all the documentation links are valid

### Using includes from the parent documentation

The most likely use case is including the `glossary.md` from the parent documentation.

To do so, use a slightly different snippet syntax:

```
!!! parent_snippet:'includes/glossary.md'
```

### Generating data for documentation

Some ehrQL documentation is generated from code in this repo.

See the [spec tests docs](tests/spec/README.md) for further information on writing tests that
contribute to the ehrQL docs.

An intermediate step generates the markdown files that are included in the documentation.

To generate this file, run:

    just generate-docs

This generates the markdown files in `docs/includes/generated_docs`.

This command runs as a pre-commit hook and will fail if there are any changes to the
generated markdown files. It is a developer's responsibility to update the generated docs in
their PR if required. There is also a CI step that will check that the documentation is up to
date.

### Updating the main OpenSAFELY documentation repository

Merges to the main branch in this repo trigger a [deployment of the main OpenSAFELY documentation via a Github Action](https://github.com/opensafely-core/ehrql/actions/workflows/deploy-documentation.yml).

### Making changes to the dataset definition snippets

These snippets are separate from the tutorial examples in `docs/ehrql-tutorial-examples`.
See [below](#ehrql-tutorial-examples) for documentation on how the tutorial examples work.
We may eventually unify the tutorial examples with the snippet
so that all example code is checked in the same way.

Edit the python modules in the `docs/snippets` directory.

Examples are included in the markdown files using the [pymdown snippet notation](https://facelessuser.github.io/pymdown-extensions/extensions/snippets/#snippets-notation).

Each of the snippets sections in each snippet Python source file are bounded by markers:

```python
# --8<-- [start:print]
print("hello world")
# --8<-- [end:print]
```

If this example was stored as `docs/snippets/hello.py`,
then it could be included in the documentation Markdown source via:

````
```python
--8<-- 'ehrql/snippets/hello.py:print'
```
````

### ehrQL tutorial examples

docs/ehrql-tutorial-examples is a collection of:

* dataset definitions
* example data
* outputs from the dataset definitions run against the example data

used in the ehrQL tutorials.

#### Adding a new example

Refer to existing ehrQL tutorial pages to see the current layout that we use.

The current process for adding a new example is:

1. Create a new dataset and add it to the [`example-data`](docs/ehrql-tutorial-examples/example-data/) directory,
2. Write the dataset definition and add it to the [`ehrql-tutorial-examples`](docs/ehrql-tutorial-examples/).
   See the [ehrQL tutorial introduction](docs/ehrql/tutorial/index.md#using-data-builders-command-line-interface)
   for an explanation of the filename convention.
3. Build the dataset definition outputs
   (see below),
   then add and commit the new files stored in the [`outputs`](docs/ehrql-tutorial-examples/outputs/) directory to the repository.
4. Use a dataset definition in the relevant documentation page's Markdown file:
   ````
   ```python title="$YOUR_DATASET_DEFINITION_FILENAME"
   ---8<-- "ehrql-tutorial-examples/$YOUR_DATASET_DEFINITION_FILENAME"
   ```
   ````
5. Use the `read_csv` feature of the [table-reader plugin](https://github.com/timvink/mkdocs-table-reader-plugin)
   in the relevant documentation page's Markdown file:
   to include the input and output CSVs as nicely formatted tables.

   You will need one `read_csv` entry for each CSV.

   Use the `keep_default_na=False` option to include blank table cells
   instead of the text `nan`.

#### Building dataset definition outputs

The easiest way is via the relevant `just` recipe.

```
just docs-build-dataset-definitions-outputs
```

#### Updating content

A GitHub Actions workflow checks that the dataset definition outputs are current.

If you modify existing example data or dataset definitions,
make sure that you:

* Commit the files you have directly changed.
* Rebuild the output CSVs and commit those if they have changed.
* Review all documentation pages that use the files you have edited,
  and check if any explanatory text requires amending as a result of your changes.


### Syncing the databuilder Docker image version with the tutorial examples

The version of the databuilder image used for the tutorials is specified in the tutorial's
[`project.yaml`](docs/ehrql-tutorial-examples/project.yaml).  We reference the image by
major version only (e.g. `v0`), so the `project.yaml` file itself only needs to be
updated rarely.  However, whenever a new image is built, it could have an impact on the
tutorial examples and their outputs.

After a merge to main, a Github Action runs to [update and test the tutorial examples](https://github.com/opensafely-core/ehrql/actions/workflows/update-tutorial-ehrql-version.yml).
If the image has changed major version, this will replace the version in the `project.yaml`. It
will also run the tutorial project via the OpenSAFELY CLI, which will generate the tutorial
output files.  If there are any changes, it will open a pull request in this repo.
