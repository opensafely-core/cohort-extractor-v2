[build-system]
requires = ["flit_core >=3.2,<4"]
build-backend = "flit_core.buildapi"

[project]
name = "opensafely-databuilder"
description = ""
version = "2+local"
readme = "README.md"
authors = [{name = "OpenSAFELY", email = "tech@opensafely.org"}]
license = {file = "LICENSE"}
classifiers = ["License :: OSI Approved :: GNU General Public License v3 or later (GPLv3+)"]
requires-python = ">=3.9"
dependencies = [
  "pandas",
  "sqlalchemy",
  "structlog",

  # Database driver for MS-SQL
  "pymssql",

  # Database driver for Spark
  "pyhive",
  # `thrift` is required for accessing Hive via `pyhive`. We could get it
  # automatically pulled in by installing `pyhive[hive]` but this will also pull
  # in `sasl` which we don't need and which causes us installation problems
  # because it doesn't have a wheel and requires the libsasl2 headers to compile.
  "thrift",
  # Databricks specific connector
  "databricks-sql-connector",
]

[project.scripts]
databuilder = "databuilder.__main__:main"

[project.urls]
Home = "https://opensafely.org"
Documentation = "https://docs.opensafely.org"
Source = "https://github.com/opensafely-core/databuilder"

[tool.black]
exclude = '''
(
  /(
      \.git         # exclude a few common directories
    | \.direnv
    | \.venv
    | venv
  )/
)
'''

[tool.coverage.run]
branch = true
omit = [
    "databuilder/dsl.py",
    "tests/acceptance/conftest.py",
    "tests/acceptance/test_long_covid_dsl.py",
    "tests/acceptance/test_med_safety_dsl.py",
    "tests/acceptance/test_sro_measures_dsl.py",
]

[tool.coverage.report]
fail_under = 100
skip_covered = true
exclude_lines = [
    # this is the default, but has to be included explicitly now we specify exclude_lines
    "pragma: no cover",
    # this indicates that the line should never be hit
    "assert False",
    # this condition is only true when a module is run as a script
    'if __name__ == "__main__":',
    # this indicates that a method should be defined in a subclass
    "raise NotImplementedError",
    # excludes the body of the overload-decorated function which will never be executed
    "@overload",
]

[tool.coverage.html]

[tool.flit.module]
name = "databuilder"

[tool.interrogate]
fail-under = 0
ignore-init-module = true
omit-covered-files = true
verbose = 1

[tool.isort]
profile = "black"
skip_glob = [".direnv", "venv", ".venv"]

[tool.mypy]
files = "databuilder"
plugins = [
  "sqlmypy",
]

# Consider turning on these stricter options later.  They require a
# significantly larger number of changes to the codebase so we have decided to
# work towards them.
# check_untyped_defs = true
# disallow_untyped_calls = true
# disallow_untyped_decorators = true
# disallow_untyped_defs = true

# https://mypy.readthedocs.io/en/stable/config_file.html#confval-disallow_incomplete_defs
disallow_incomplete_defs = true

# https://mypy.readthedocs.io/en/stable/config_file.html#confval-no_implicit_optional
no_implicit_optional = true

# https://mypy.readthedocs.io/en/stable/config_file.html#confval-warn_redundant_casts
warn_redundant_casts = true

# https://mypy.readthedocs.io/en/stable/config_file.html#confval-warn_unused_ignores
warn_unused_ignores = true

# https://mypy.readthedocs.io/en/stable/config_file.html#confval-warn_unreachable
warn_unreachable = true

# https://mypy.readthedocs.io/en/stable/config_file.html#confval-scripts_are_modules
scripts_are_modules = true

# https://mypy.readthedocs.io/en/stable/config_file.html#confval-show_error_codes
show_error_codes = true

[[tool.mypy.overrides]]
module = [
  "pyhive.sqlalchemy_hive.*",
  "sqlalchemy.dialects.mssql.pymssql",
  "databricks",
  "databricks.sql",
]
ignore_missing_imports = true

[tool.pydocstyle]
convention = "google"
add_select = [
  "D213",
]
# Base ignores for all docstrings, for module/package specific ones add them to
# the CLI args in Justfile
add_ignore = [
  "D100",
  "D104",
  "D107",
  "D212",
]

[tool.pytest.ini_options]
addopts = "--tb=native --strict-markers"
testpaths = ["tests"]
markers = [
    "integration: tests that use a container (mostly databases)",
    "spark: tests that use a Spark database (and therefore tend to be very slow)",
]
filterwarnings = [
    "ignore::DeprecationWarning:past.builtins.misc:45",
    "ignore::DeprecationWarning:pytest_freezegun:17",
    "ignore::DeprecationWarning:docker.utils.utils:52",
    "ignore::DeprecationWarning:docker.utils.utils:53",
]
