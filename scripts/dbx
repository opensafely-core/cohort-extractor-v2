#!/usr/bin/env python3
import configparser
import os
import subprocess
import sys
import time
from urllib.parse import urljoin

import click
import requests

PROFILE = os.environ.get("DATABRICKS_PROFILE", "DEFAULT")
CLUSTERS = None
ORGID = None
USERNAME = None
PASSWORD = None
HOST = None
COMMANDS = {}
OUTPUT = True

VENV = os.environ["VIRTUAL_ENV"]
DATABRICKS_EXEC = f"{VENV}/bin/databricks"
PYTHON = f"{VENV}/bin/python"


def run(cmd, check=True, text=True, **kwargs):
    echo(" ".join(cmd).replace(PASSWORD, "*****"))
    return subprocess.run(cmd, check=check, text=text, **kwargs)


def databricks(cmd, *args, **kwargs):
    cmd = [DATABRICKS_EXEC, "--profile", PROFILE] + cmd
    return run(cmd, *args, **kwargs)


def is_community_edition():
    return "community.cloud.databricks.com" in HOST


def echo(*args, **kwargs):
    if OUTPUT:
        click.echo(*args, **kwargs)


class Cluster:
    def __init__(self, data):
        self.data = data

    @property
    def name(self):
        return self.data["cluster_name"]

    @property
    def cluster_id(self):
        return self.data["cluster_id"]

    @property
    def state(self):
        return self.data["state"]

    @property
    def url(self):
        host_with_creds = HOST.replace("https://", f"https://{USERNAME}:{PASSWORD}@")
        return f"{host_with_creds}/default?http_path=sql/protocolv1/o/{ORGID}/{self.cluster_id}"

    def __str__(self):
        return f"{self.cluster_id} ({self.name}): {self.state}"


def get_auth():
    global USERNAME, PASSWORD, HOST
    path = os.path.expanduser("~/.databrickscfg")
    if not os.path.exists(path):
        sys.exit("databricks cli has not been configured. Run:\ndatabricks configure")
    parser = configparser.ConfigParser()
    parser.read(path)
    profile = parser[PROFILE]
    USERNAME = profile["username"]
    PASSWORD = profile["password"]
    HOST = profile["host"]


def get_clusters():
    # we hit the API directly here, as we need access to the headers.
    global CLUSTERS, ORGID

    resp = requests.get(
        urljoin(HOST, "/api/2.0/clusters/list"),
        auth=(USERNAME, PASSWORD),
    )
    resp.raise_for_status()

    all_clusters = resp.json().get("clusters", [])
    if all_clusters:
        echo("Fetched clusters:")
        echo(
            "\n".join(
                [
                    f'{cl["cluster_name"]} | {cl["cluster_id"]} | {cl["state"]}'
                    for cl in all_clusters
                ]
            )
        )
    # All possible clusters states
    STATES = [
        "RUNNING",
        "PENDING",
        "RESTARTING",
        "RESIZING",
        "TERMINATING",
        "TERMINATED",
    ]
    # There can be more than one cluster with the same name
    # Sort clusters by state and then cluster ID; if there are multiple clusters with the
    # same name and different states, this will return the running one first, and a pending
    # one before a terminated one.  This means that if we retrieved one of two terminated
    # clusters in order to start one up, when we fetch them again we'll find the one that's
    # been started up
    # We reverse the sorted list, such that RUNNING is last and TERMINATED is first.
    # We then instantiate a Cluster object for each state/name combination,
    # but overwrite earlier instances with later instances, for instances with
    # the same name.
    all_clusters.sort(
        key=lambda c: (STATES.index(c["state"]), c["cluster_id"]), reverse=True
    )
    CLUSTERS = {
        c["cluster_name"]: Cluster(c) for c in all_clusters if c["cluster_name"]
    }
    ORGID = resp.headers["X-Databricks-Org-Id"]


def _wait(name, timeout=90):
    echo(f"Waiting for cluster:\n{CLUSTERS[name]}")
    start = time.time()
    while CLUSTERS[name].state in ["PENDING", "RESTARTING"]:
        if time.time() - start > timeout:
            sys.exit(f"timed out after {timeout}s waiting for cluster {name}")
        time.sleep(10)
        get_clusters()


@click.group()
def cli():
    """Command for managing databricks cluster.

    The default cluster name is 'opensafely-test', but can be overridden with
    --name option or the DATABRICKS_CLUSTER env var.

    It will ensure the cluster is running, but will not create it if it doesn't already exist.

    By default, it uses the DEFAULT profile of your databricks cli config, but
    this can be altered with the DATABRICKS_PROFILE env var.
    """
    pass


# common option for many commands
name_option = click.option("--name", default="opensafely-test", help="name of cluster")


@cli.command()
@name_option
@click.option(
    "--wait/--no-wait",
    default=False,
    help="wait for cluster to be available before exiting",
)
@click.option("--timeout", default=90, help="timeout to wait if --wait is passed")
@click.option(
    "--output-url/--no-output-url", default=False, help="only output the connection url"
)
def start(name, wait, output_url, timeout):
    """Ensure cluster is running."""
    global OUTPUT

    if output_url:
        OUTPUT = False

    c = CLUSTERS.get(name)

    # it exists
    if c:
        # it's running or starting up
        if c.state in ("PENDING", "RUNNING", "RESTARTING"):
            if output_url:
                click.echo(c.url)
            else:
                echo(f"Target cluster:\n{c}")
            if c.state == "RUNNING":
                return
        else:
            # cluster exists, but is not running or starting up
            click.echo(f"Starting cluster {c.cluster_id}")
            databricks(["clusters", "start", "--cluster-id", c.cluster_id])
    else:
        click.echo(
            f"No {name} cluster found.\n"
            f"You will need to create one called '{name}' via the web UI."
        )
        sys.exit(1)

    get_clusters()

    if wait:
        _wait(name, timeout)

    if output_url:
        click.echo(CLUSTERS[name].url)
    else:
        echo(CLUSTERS[name])


@cli.command()
@name_option
def wait(name):
    """Wait until cluster is available."""
    _wait(name)


@cli.command()
@name_option
def url(name):
    """Print connection string."""
    c = CLUSTERS.get(name)
    if c:
        click.echo(c.url)
    else:
        click.echo(f"No cluster named {name}")


@cli.command()
@name_option
def test(name):
    """Test cluster up and working."""
    _wait(name)
    c = CLUSTERS[name]
    run(
        [
            PYTHON,
            "-m",
            "databuilder",
            "test-connection",
            "-b",
            "databuilder.backends.databricks.DatabricksBackend",
            "-u",
            c.url,
        ]
    )


@cli.command()
@name_option
def status(name):
    """Print cluster status."""
    if name in CLUSTERS:
        click.echo(CLUSTERS[name])
    else:
        click.echo(f"Cluster {name} not found")


@cli.command()
def cleanup():
    """Remove all data on the cluster."""
    if is_community_edition():
        databricks(
            [
                "workspace",
                "import",
                "-l",
                "python",
                "-o",
                "scripts/dbx-cleanup.py",
                "/cleanup",
            ]
        )
        click.echo(
            "The cleanup notebook has been uploaded to your Databricks Workspace.\n"
            "Unfortunately, it cannot be executed remotely in Community Edition.\n"
            "Log in to https://community.cloud.databricks.com and select Workspace -> cleanup and run it manually."
        )
    else:
        ps = databricks(["fs", "ls", "dbfs:/user/hive/warehouse/"], capture_output=True)
        for db in ps.stdout.split():
            databricks(["fs", "rm", "-r", f"dbfs:/user/hive/warehouse/{db}"])


if __name__ == "__main__":
    get_auth()
    get_clusters()
    cli()
